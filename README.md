# ü¶ü DataEng_Dengue

Projeto de Engenharia de Dados para an√°lise e monitoramento de casos de **Dengue** a partir de dados p√∫blicos do **DATASUS**. O objetivo √© estruturar um pipeline completo de ETL utilizando **Apache Airflow**, armazenamento em **SQLite** e extra√ß√£o de insights por meio de **consultas SQL**.

## üìä Dados

- **Fonte dos dados:**  
  [DATASUS - SINAN Dengue](https://opendatasus.saude.gov.br/dataset/arboviroses-dengue)

  [C√≥digo Munic√≠pios - IBGE](https://www.ibge.gov.br/explica/codigos-dos-municipios.php)
- **Descri√ß√£o:**  
  Os dados utilizados s√£o oriundos do **Sistema de Informa√ß√£o de Agravos de Notifica√ß√£o (SINAN)**, especificamente focados nos casos de **Dengue notificados em Pernambuco**. O dataset cont√©m vari√°veis cl√≠nicas, sociodemogr√°ficas e geogr√°ficas, incluindo sintomas, comorbidades, sinais de gravidade, entre outros.

- **Por que esses dados?**  
  A escolha foi motivada pela relev√¢ncia **epidemiol√≥gica e social** da dengue no Brasil. Al√©m disso, trata-se de uma base p√∫blica, rica e padronizada, com potencial para gerar an√°lises que subsidiem **pol√≠ticas de sa√∫de p√∫blica**, especialmente sobre **popula√ß√µes de risco** e **efici√™ncia na vigil√¢ncia epidemiol√≥gica**.

## ‚öôÔ∏è Extra√ß√£o e Transforma√ß√£o

O pipeline de ETL foi orquestrado utilizando o **Apache Airflow**, e o fluxo pode ser resumido conforme o diagrama abaixo:

```mermaid
flowchart TD
    %% Estilo dos n√≥s
    classDef airflow fill:#cce5ff,stroke:#007bff,stroke-width:1px,color:#000;
    classDef data fill:#d4edda,stroke:#28a745,stroke-width:1px,color:#000;
    classDef module fill:#ffeeba,stroke:#ffc107,stroke-width:1px,color:#000;
    classDef external fill:#343a40,stroke:#6c757d,stroke-width:1px,color:#fff;
    classDef container fill:#6c757d,stroke:#343a40,stroke-width:1px,color:#fff;
    classDef notify fill:#dc3545,stroke:#721c24,stroke-width:1px,color:#fff;

    %% Componentes externos
    DATASUS["DATASUS - FTP/HTTP Endpoint"]:::external

    %% Orquestra√ß√£o - Apache Airflow
    subgraph Airflow
        Scheduler["Airflow Scheduler"]:::airflow
        Webserver["Webserver UI"]:::airflow
        Worker["Worker / Executor"]:::airflow
    end

    %% DAG principal
    Scheduler --> DAG["ETL DAG"]

    %% M√≥dulos do DAG
    DAG --> Verifica["verificar_atualizacao.py"]:::module
    Verifica --> |Atualiza| UltimaDataTxt["ultima_data.txt"]:::data
    Verifica -->|Se h√° novos dados| Extract["extract.py"]:::module
    
    Extract -->|Extrai dados de| DATASUS
    Extract --> |Executa| Dimensoes["dimensoes.py"]:::module
    Extract -->|Salva dados em| SQLiteDB["SQLite: dados_sinan.db"]:::data
    
    Dimensoes --> JSONUF["municipios.json"]:::data
    Dimensoes --> JSONMun["uf.json"]:::data
    Dimensoes -->|Transforma e carrega em| SQLiteDB

    Verifica --> Email["enviar_email.py"]:::module
    Email -->|Envia resumo via| SMTP["SMTP Server"]:::notify

    %% Volumes compartilhados
    subgraph Data
        SQLiteDB["SQLite: dados_sinan.db"]:::data
        JSONUF["municipios.json"]:::data
        JSONMun["uf.json"]:::data
        UltimaDataTxt["ultima_data.txt"]:::data
    end

    %% Containeriza√ß√£o
    Dockerfile["Dockerfile"]:::container
    Compose["docker-compose.yaml"]:::container
    Dockerfile -->|Cria imagem| Airflow
    Compose -->|Orquestra servi√ßos e volumes| Airflow
```

## üíΩ Tecnologias utilizadas

- **Apache Airflow** para orquestra√ß√£o de tarefas  
- **Python** para scripts de extra√ß√£o, transforma√ß√£o e carga  
- **SQLite** como banco de dados local para persist√™ncia  
- **Docker** e **Docker Compose** para containeriza√ß√£o do ambiente  

---

## üöÄ Como subir a aplica√ß√£o com Docker

1. **Suba os containers** com o seguinte comando:

   ```bash
   docker-compose up -d

2. Aguarde alguns segundos enquanto o Apache Airflow inicializa os servi√ßos.

3. Acesse o Airflow atrav√©s do navegador:

    http://localhost:8080

4. Use as seguintes credenciais para login:

    Usu√°rio: airflow

    Senha: airflow

## üõ†Ô∏è Dica para testar a extra√ß√£o novamente
Se quiser ver a extra√ß√£o de dados acontecendo novamente, voc√™ pode for√ßar isso excluindo o arquivo ultima_data.txt, que est√° localizado na pasta **data/ultima_data.txt**
Esse arquivo armazena a √∫ltima data de extra√ß√£o realizada no site do SINAN. Ao remov√™-lo, o DAG (quando iniciar) ir√° considerar que uma nova extra√ß√£o precisa ser feita.

## üîç Consultas SQL
# 1. Casos com Comorbidades Graves
- **Objetivo:**
Identificar pacientes com hospitaliza√ß√£o, comorbidades (diabetes, hipertens√£o, hepatopatias) ou sinais de gravidade, possibilitando o monitoramento de popula√ß√µes vulner√°veis e demanda por leitos hospitalares.

- **Por que essa consulta?**
Oferece uma vis√£o cr√≠tica sobre os casos mais graves e complexos, permitindo uma estratifica√ß√£o de risco e tomada de decis√£o cl√≠nica/estrat√©gica.

# 2. Tempo entre Sintomas e Notifica√ß√£o
- **Objetivo:**
Calcular o n√∫mero de dias entre a data dos primeiros sintomas e a data da notifica√ß√£o, avaliando a efici√™ncia da resposta da vigil√¢ncia epidemiol√≥gica.

- **Por que essa consulta?**
Permite diagnosticar atrasos na comunica√ß√£o entre os servi√ßos de sa√∫de e detectar poss√≠veis gargalos na cadeia de notifica√ß√£o.

## üèÅ Desafios e melhorias futuras
- A estrutura dos dados do DATASUS √© complexa, fragmentada em diferentes dicion√°rios e formatos. Foi necess√°rio um esfor√ßo inicial para encontrar uma boa base de dados, decodificar os dados categ√≥ricos e relacionar com suas dimens√µes descritivas.

- Um dos principais obst√°culos foi a extra√ß√£o de um grande volume de dados (+25GB). Inicialmente, rodando localmente ainda era poss√≠vel armazenar os dados em mem√≥ria. Mas, depois de construir a aplica√ß√£o completa do Airflow isso se tornou um problema. A solu√ß√£o seria adicionar uma camada de ferramentas para big data como pyspark.

- Substituir o SQLite por um banco mais robusto dependendo da arquitetura do time.
